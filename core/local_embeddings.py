import os
import hashlib
import numpy as np
from typing import List, Dict, Any
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma


class LocalEmbeddings:
    """–ü–æ–ª–Ω–æ—Å—Ç—å—é –ª–æ–∫–∞–ª—å–Ω—ã–π embeddings –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π"""
    
    def __init__(self, dimension=384):
        self.dimension = dimension
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω –ª–æ–∫–∞–ª—å–Ω—ã–π embeddings —Å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é {dimension}")
    
    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """–°–æ–∑–¥–∞–µ—Ç embeddings –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        embeddings = []
        for text in texts:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º SHA-256 –¥–ª—è –ª—É—á—à–µ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
            hash_obj = hashlib.sha256(text.encode('utf-8'))
            hash_bytes = hash_obj.digest()
            
            # –°–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä –Ω—É–∂–Ω–æ–π —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏
            vector = []
            for i in range(self.dimension):
                if i < len(hash_bytes):
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –æ—Ç 0 –¥–æ 1
                    vector.append(float(hash_bytes[i % len(hash_bytes)]) / 255.0)
                else:
                    vector.append(0.0)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–µ–∫—Ç–æ—Ä
            norm = np.linalg.norm(vector)
            if norm > 0:
                vector = [v / norm for v in vector]
            
            embeddings.append(vector)
        
        return embeddings
    
    def embed_query(self, text: str) -> List[float]:
        """–°–æ–∑–¥–∞–µ—Ç embeddings –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞"""
        return self.embed_documents([text])[0]


class LocalVectorStore:
    """–ü—Ä–æ—Å—Ç–æ–µ –ª–æ–∫–∞–ª—å–Ω–æ–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ"""
    
    def __init__(self, documents: List[Dict], embeddings: LocalEmbeddings):
        self.documents = documents
        self.embeddings = embeddings
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        doc_contents = []
        for doc in documents:
            if hasattr(doc, 'page_content'):
                doc_contents.append(doc.page_content)
            elif isinstance(doc, dict) and 'page_content' in doc:
                doc_contents.append(doc['page_content'])
            else:
                doc_contents.append(str(doc))
        
        self.doc_embeddings = embeddings.embed_documents(doc_contents)
        print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ –ª–æ–∫–∞–ª—å–Ω–æ–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ —Å {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏")
    
    def similarity_search(self, query: str, k: int = 3) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        query_embedding = self.embeddings.embed_query(query)
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        similarities = []
        for i, doc_embedding in enumerate(self.doc_embeddings):
            similarity = np.dot(query_embedding, doc_embedding)
            similarities.append((similarity, i))
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–¥—Å—Ç–≤–∞
        similarities.sort(reverse=True)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-k –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        results = []
        for similarity, idx in similarities[:k]:
            results.append(self.documents[idx])
        
        return results


def init_local_vector_store():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –ª–æ–∫–∞–ª—å–Ω–æ–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ"""
    print("üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞...")
    
    # –°–æ–∑–¥–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π embeddings
    embeddings = LocalEmbeddings()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–æ–π
    try:
        loader = DirectoryLoader(
            "knowledge_base/",
            glob="**/*.txt",
            loader_cls=TextLoader,
            loader_kwargs={"encoding": "utf-8"},
            show_progress=True,
            use_multithreading=False  # –û—Ç–∫–ª—é—á–∞–µ–º –º–Ω–æ–≥–æ–ø–æ—Ç–æ—á–Ω–æ—Å—Ç—å –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
        )
        docs = loader.load()
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    except Exception as e:
        print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {e}")
        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        from langchain.schema import Document
        docs = [
            Document(page_content="–¢–µ—Å—Ç–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –æ –ø—Ä–æ–¥—É–∫—Ç–∞—Ö –∫–æ–º–ø–∞–Ω–∏–∏", metadata={"source": "test"}),
            Document(page_content="–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –¥–æ—Å—Ç–∞–≤–∫–µ –∏ –≥–∞—Ä–∞–Ω—Ç–∏–∏", metadata={"source": "test"}),
            Document(page_content="–¶–µ–Ω—ã –∏ —Å–∫–∏–¥–∫–∏ –Ω–∞ —Ç–æ–≤–∞—Ä—ã", metadata={"source": "test"})
        ]
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ —á–∞–Ω–∫–∏
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50,
        length_function=len
    )
    chunks = splitter.split_documents(docs)
    print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(chunks)} —á–∞–Ω–∫–æ–≤")
    
    # –°–æ–∑–¥–∞–µ–º –ª–æ–∫–∞–ª—å–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
    vector_store = LocalVectorStore(chunks, embeddings)
    
    # –°–æ–∑–¥–∞–µ–º retriever-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π —Å LangChain
    class LocalRetriever:
        def __init__(self, vector_store):
            self.vector_store = vector_store
        
        def get_relevant_documents(self, query: str) -> List[Dict]:
            return self.vector_store.similarity_search(query, k=3)
        
        def __call__(self, query: str) -> List[Dict]:
            return self.get_relevant_documents(query)
        
        @property
        def vectorstore(self):
            return self.vector_store
    
    retriever = LocalRetriever(vector_store)
    # –î–æ–±–∞–≤–ª—è–µ–º –∞—Ç—Ä–∏–±—É—Ç—ã –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å LangChain
    retriever.search_type = "similarity"
    retriever.search_kwargs = {"k": 3}
    
    return retriever 